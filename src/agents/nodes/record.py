import os
import sys
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

if project_root not in sys.path:
    sys.path.append(project_root)

import torch
import json
import gc
import time
from transformers import pipeline
import google.generativeai as genai
import static_ffmpeg
static_ffmpeg.add_paths()
# --- C·∫§U H√åNH ---
MODEL_ID = "openai/whisper-large-v3-turbo" # B·∫£n v3-turbo r·∫•t nhanh v√† ch√≠nh x√°c cho ti·∫øng Vi·ªát
OUTPUT_DIR = "output_data"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Kh·ªüi t·∫°o Pipeline (D√πng c∆° ch·∫ø Global ƒë·ªÉ tr√°nh load l·∫°i model nhi·ªÅu l·∫ßn)
_asr_pipeline = None

def get_asr_pipeline():
    global _asr_pipeline
    if _asr_pipeline is None:
        print(f"‚è≥ ƒêang kh·ªüi t·∫°o model Whisper t·ª´ Hugging Face ({MODEL_ID})...")
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        _asr_pipeline = pipeline(
            "automatic-speech-recognition",
            model=MODEL_ID,
            torch_dtype=torch_dtype,
            device=device,
            model_kwargs={"attn_implementation": "sdpa"} if torch.cuda.is_available() else {}
        )
    return _asr_pipeline

# --- C√ÅC H√ÄM X·ª¨ L√ù ---

def _transcribe_with_transformers(file_path):
    """
    S·ª≠ d·ª•ng Transformers Pipeline v·ªõi c∆° ch·∫ø chunking t·ª± ƒë·ªông.
    Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ tr√†n RAM v√† file √¢m thanh d√†i.
    """
    pipe = get_asr_pipeline()
    
    print(f"üéôÔ∏è ƒêang nh·∫≠n di·ªán gi·ªçng n√≥i: {os.path.basename(file_path)}...")
    
    # generate_kwargs gi√∫p ƒë·ªãnh h∆∞·ªõng ng√¥n ng·ªØ v√† thu·∫≠t ng·ªØ
    # L∆∞u √Ω: Whisper c·ªßa Transformers kh√¥ng nh·∫≠n 'prompt' m·∫°nh nh∆∞ Gemini 
    # nh∆∞ng 'return_timestamps' gi√∫p track n·ªôi dung t·ªët h∆°n.
    result = pipe(
        file_path,
        chunk_length_s=30,      # T·ª± ƒë·ªông c·∫Øt m·ªói 30s ƒë·ªÉ x·ª≠ l√Ω
        batch_size=8,           # X·ª≠ l√Ω song song 8 ƒëo·∫°n (tƒÉng t·ªëc ƒë·ªô)
        return_timestamps=True,
        generate_kwargs={"language": "vietnamese", "task": "transcribe"}
    )
    
    return result["text"]

def process_audio_v2(file_path: str, plan: dict):
    """
    Pipeline: Audio -> Transformers (Text) -> Gemini (JSON)
    """
    # 1. Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh vƒÉn b·∫£n
    raw_text = _transcribe_with_transformers(file_path)
    
    # 2. C·∫•u h√¨nh Gemini
    from config import GEMINI_API_KEY
    genai.configure(api_key=GEMINI_API_KEY)
    
    output_format = plan.get('output_format', 'SUMMARY').upper()
    quantity = plan.get('quantity', 5)

    # System Prompt cho Gemini
    if output_format == "FLASHCARD":
        system_instruction = (
            f"B·∫°n l√† chuy√™n gia so·∫°n th·∫£o h·ªçc li·ªáu. H√£y tr√≠ch xu·∫•t {quantity} ki·∫øn th·ª©c quan tr·ªçng nh·∫•t t·ª´ t√†i li·ªáu n√†y ƒë·ªÉ t·∫°o flashcards."
        )
        # √âp ki·ªÉu JSON cho Flashcard
        prompt = "Tr·∫£ v·ªÅ danh s√°ch JSON array: [{\"question\": \"...\", \"answer\": \"...\"}]"
    else:
        system_instruction = (
            "B·∫°n l√† tr·ª£ l√Ω t√≥m t·∫Øt t√†i li·ªáu. H√£y ph√¢n t√≠ch c·∫•u tr√∫c c·ªßa slide v√† t·∫°o b·∫£n t√≥m t·∫Øt theo t·ª´ng ch∆∞∆°ng ho·∫∑c m·ª•c l·ªõn m·ªôt c√°ch logic."
        )
        # √âp ki·ªÉu JSON cho Summary
        prompt = """
        Tr·∫£ v·ªÅ theo c·∫•u tr√∫c: 
        {
            "title": "Ti√™u ƒë·ªÅ t√†i li·ªáu",
            "outline": [{"heading": "T√™n ph·∫ßn", "summary": "N·ªôi dung t√≥m t·∫Øt"}],
            "conclusion": "K·∫øt lu·∫≠n ch√≠nh"
        }
        """

    # 3. G·ªçi Gemini x·ª≠ l√Ω text (V√¨ text ƒë√£ c√≥ s·∫µn, kh√¥ng c·∫ßn upload file l√™n File API tr·ª´ khi text qu√° d√†i)
    model = genai.GenerativeModel("gemini-2.5-flash")
    
    # N·∫øu text qu√° d√†i (> 30.000 t·ª´), n√™n l∆∞u ra file r·ªìi upload. 
    # N·∫øu ng·∫Øn, g·ª≠i tr·ª±c ti·∫øp trong message:
    response = model.generate_content(
        [raw_text, system_instruction, prompt],
        generation_config={"response_mime_type": "application/json"}
    )

    return json.loads(response.text)

# --- LANGGRAPH NODE ---

def audio_processor_node(state: dict):
    print(f"\n--- [TRANSFORMERS AGENT] X·ª¨ L√ù: {state.get('user_intent')} ---")
    
    file_path = state.get("input_data")
    if not file_path or not os.path.exists(file_path):
        return {"answer": "‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file audio."}

    try:
        plan = {
            "output_format": state.get("user_intent", "SUMMARY"),
            "quantity": state.get("quantity", 5)
        }
        
        result_json = process_audio_v2(file_path, plan)
        
        # L∆∞u k·∫øt qu·∫£
        file_id = os.path.splitext(os.path.basename(file_path))[0]
        save_path = os.path.join(OUTPUT_DIR, f"{file_id}_{plan['output_format'].lower()}.json")
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(result_json, f, ensure_ascii=False, indent=4)

        return {
            "answer": json.dumps(result_json, ensure_ascii=False, indent=2),
            "context": [f"D√πng model: {MODEL_ID}", f"L∆∞u t·∫°i: {save_path}"]
        }
    except Exception as e:
        return {"answer": f"L·ªói pipeline: {str(e)}"}
    

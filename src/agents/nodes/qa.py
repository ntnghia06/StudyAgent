import os
import sys

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

if project_root not in sys.path:
    sys.path.append(project_root)


import numpy as np
import google.generativeai as genai
from typing import Dict, Any
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Import model embedding tá»« manager cá»§a báº¡n
from database.qdrant_manager import get_embedding_model
from config import GEMINI_API_KEY

# Cáº¥u hÃ¬nh Gemini
genai.configure(api_key=GEMINI_API_KEY)

def cosine_similarity(v1, v2):
    """TÃ­nh toÃ¡n tÆ°Æ¡ng Ä‘á»“ng Cosine giá»¯a vector cÃ¢u há»i vÃ  máº£ng cÃ¡c vector chunks."""
    # Chuyá»ƒn vá» numpy Ä‘á»ƒ tÃ­nh toÃ¡n ma tráº­n cho nhanh
    v1 = np.array(v1)
    v2 = np.array(v2)
    
    dot_product = np.dot(v2, v1)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2, axis=1)
    
    return dot_product / (norm_v1 * norm_v2)

def qa_node(state: Dict[str, Any]):
    """
    NODE: Chunk text tá»« state -> Search cá»¥c bá»™ -> Gá»i API tráº£ lá»i.
    Input state: { "answer": "vÄƒn báº£n thÃ´...", "query": "cÃ¢u há»i..." }
    """
    print("--- ğŸ§  ÄANG Xá»¬ LÃ TRUY Váº¤N Cá»¤C Bá»˜ (NO-DB RAG) ---")
    
    # 1. Láº¥y dá»¯ liá»‡u Ä‘áº§u vÃ o tá»« state
    raw_text = state.get("answer", "")
    query = state.get("query", "")
    if isinstance(raw_text, dict):
        # Náº¿u nÃ³ lÃ  JSON tá»« Gemini, thÆ°á»ng vÄƒn báº£n náº±m trong má»™t key nÃ o Ä‘Ã³, 
        # hoáº·c ta convert toÃ n bá»™ sang string Ä‘á»ƒ chunk
        print("âš ï¸ Cáº£nh bÃ¡o: raw_text Ä‘ang lÃ  Dict, Ä‘ang chuyá»ƒn sang String...")
        # CÃ¡ch 1: Láº¥y trÆ°á»ng 'content' náº¿u cÃ³
        # raw_text = raw_text.get("content", str(raw_text)) 
        # CÃ¡ch 2: Chuyá»ƒn toÃ n bá»™ dict thÃ nh chuá»—i JSON
        import json
        raw_text = json.dumps(raw_text, ensure_ascii=False)

    if not isinstance(raw_text, str) or not raw_text.strip():
        return {"answer": "âŒ Lá»—i: KhÃ´ng cÃ³ vÄƒn báº£n há»£p lá»‡ Ä‘á»ƒ xá»­ lÃ½."}
    if not raw_text or not query:
        return {"answer": "âŒ Lá»—i: Thiáº¿u vÄƒn báº£n nguá»“n hoáº·c cÃ¢u há»i trong state."}

    # 2. Chunking: Chia nhá» vÄƒn báº£n
    # NghÄ©a nÃªn Ä‘á»ƒ chunk_size vá»«a pháº£i Ä‘á»ƒ Gemini nháº­n Ä‘á»§ ngá»¯ cáº£nh
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=150,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_text(raw_text)
    
    if not chunks:
        return {"answer": "âš ï¸ KhÃ´ng thá»ƒ chia nhá» vÄƒn báº£n (vÄƒn báº£n quÃ¡ ngáº¯n hoáº·c rá»—ng)."}

    # 3. Local Semantic Search (Sá»­ dá»¥ng model 384-dim cá»§a báº¡n)
    try:
        embeddings_model = get_embedding_model()
        
        # Chuyá»ƒn query vÃ  toÃ n bá»™ chunks thÃ nh vectors
        query_vec = embeddings_model.embed_query(query)
        chunk_vecs = embeddings_model.embed_documents(chunks)
        
        # TÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng vÃ  láº¥y top 3
        scores = cosine_similarity(query_vec, chunk_vecs)
        top_k_indices = np.argsort(scores)[-3:][::-1] # Láº¥y 3 cÃ¡i cao nháº¥t
        relevant_context = [chunks[i] for i in top_k_indices]
        
    except Exception as e:
        print(f"âŒ Lá»—i xá»­ lÃ½ Vector: {e}")
        relevant_context = chunks[:3] # Fallback: láº¥y Ä‘áº¡i 3 Ä‘oáº¡n Ä‘áº§u

    # 4. API Call: Gá»­i context vÃ  query lÃªn Gemini
    context_combined = "\n\n---\n\n".join(relevant_context)
    
    system_instruction = (
        "Báº¡n lÃ  trá»£ lÃ½ há»c thuáº­t chuyÃªn sÃ¢u. HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p. "
        "Giá»¯ nguyÃªn thuáº­t ngá»¯ chuyÃªn ngÃ nh tiáº¿ng Anh vÃ  dÃ¹ng LaTeX cho cÃ´ng thá»©c toÃ¡n/tin."
    )
    
    prompt = f"""
    NGá»® Cáº¢NH TRÃCH XUáº¤T:
    {context_combined}
    
    CÃ‚U Há»I:
    {query}
    
    YÃŠU Cáº¦U: HÃ£y phÃ¢n tÃ­ch dá»±a trÃªn ngá»¯ cáº£nh trÃªn vÃ  tráº£ lá»i chi tiáº¿t.
    """

    model = genai.GenerativeModel("gemini-2.5-flash", system_instruction=system_instruction)
    
    try:
        response = model.generate_content(
            prompt,
            generation_config={"temperature": 0.3}
        )
        
        # Tráº£ vá» state má»›i vá»›i cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng
        return {
            "answer": response.text,
            "context": relevant_context # LÆ°u láº¡i Ä‘á»ƒ debug hoáº·c hiá»ƒn thá»‹ nguá»“n
        }
    except Exception as e:
        return {"answer": f"âŒ Lá»—i API: {str(e)}"}
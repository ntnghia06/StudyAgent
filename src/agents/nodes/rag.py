import os
import sys

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

if project_root not in sys.path:
    sys.path.append(project_root)

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from src.database.vector_store import query_vector_db
from dotenv import load_dotenv
load_dotenv()

# 1. Cáº¥u hÃ¬nh Model Gemini
# Äáº£m báº£o báº¡n Ä‘Ã£ Ä‘áº·t GOOGLE_API_KEY trong biáº¿n mÃ´i trÆ°á»ng
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=os.getenv("GEMINI_API_KEY"),
    temperature=0.3, # Tháº¥p Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c, trÃ¡nh "áº£o giÃ¡c"
)

# 2. XÃ¢y dá»±ng Prompt máº«u (Instruction)
SYSTEM_PROMPT = """
Báº¡n lÃ  má»™t Trá»£ lÃ½ Há»c táº­p thÃ´ng minh (Study Agent). 
Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn cÃ¡c tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y.

YÃŠU Cáº¦U:
- Chá»‰ sá»­ dá»¥ng thÃ´ng tin trong pháº§n "NGá»® Cáº¢NH" Ä‘á»ƒ tráº£ lá»i.
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin trong ngá»¯ cáº£nh, hÃ£y nÃ³i "Xin lá»—i, kiáº¿n thá»©c nÃ y chÆ°a cÃ³ trong dá»¯ liá»‡u há»c táº­p cá»§a báº¡n".
- Tráº£ lá»i ngáº¯n gá»n, sÃºc tÃ­ch vÃ  dá»… hiá»ƒu theo phong cÃ¡ch há»c thuáº­t.

NGá»® Cáº¢NH:
{context}
"""

def get_answer(state: dict): 
    """Quy trÃ¬nh RAG: TrÃ­ch xuáº¥t query tá»« state -> TÃ¬m kiáº¿m -> Tráº£ lá»i."""
    
    # BÆ¯á»šC 0: Láº¥y chuá»—i vÄƒn báº£n thá»±c sá»± tá»« trong state ra
    # Trong AgentState cá»§a báº¡n, cÃ¢u há»i náº±m á»Ÿ trÆ°á»ng "query"
    user_query = state.get("query", "") 
    
    # Kiá»ƒm tra náº¿u query trá»‘ng (Ä‘á»ƒ trÃ¡nh lá»—i embedding)
    if not user_query:
        # Náº¿u chÆ°a cÃ³ query, cÃ³ thá»ƒ láº¥y tá»« input_data
        user_query = state.get("input_data", "")

    print(f"--- ğŸ” ÄANG TÃŒM KIáº¾M CHO CÃ‚U Há»I: {user_query} ---")

    # BÆ¯á»šC 1: TÃ¬m kiáº¿m tÃ i liá»‡u (LÃºc nÃ y user_query cháº¯c cháº¯n lÃ  String)
    relevant_docs = query_vector_db(user_query, k=3)
    
    context = "\n".join([doc.page_content for doc in relevant_docs])
    
    if not context:
        return {"answer": "TÃ i liá»‡u há»c táº­p cá»§a báº¡n hiá»‡n Ä‘ang trá»‘ng hoáº·c khÃ´ng cÃ³ thÃ´ng tin liÃªn quan."}

    # BÆ¯á»šC 2: Táº¡o Prompt (Giá»¯ nguyÃªn logic cá»§a báº¡n)
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        ("human", "{question}")
    ])
    
    # BÆ¯á»šC 3: Gá»i AI táº¡o cÃ¢u tráº£ lá»i
    chain = prompt | llm
    response = chain.invoke({
        "context": context,
        "question": user_query # Truyá»n string vÃ o Ä‘Ã¢y
    })
    
    # BÆ¯á»šC 4: TRáº¢ Vá»€ báº£n cáº­p nháº­t cho State (Dáº¡ng Dictionary)
    # LangGraph sáº½ láº¥y giÃ¡ trá»‹ nÃ y Ä‘á»ƒ cáº­p nháº­t vÃ o trÆ°á»ng "answer" trong AgentState
    print(response.content)
    return {
        "answer": response.content,
        "context": context # LÆ°u luÃ´n context vÃ o state Ä‘á»ƒ tiá»‡n debug náº¿u cáº§n
    }

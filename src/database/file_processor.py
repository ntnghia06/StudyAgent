import os
import sys
import fitz  # PyMuPDF
import docx  # python-docx
from langchain_text_splitters import RecursiveCharacterTextSplitter

# X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.append(project_root)

from src.database.vector_store import add_to_vector_db

def extract_text_from_word(docx_path):
    """Tr√≠ch xu·∫•t to√†n b·ªô vƒÉn b·∫£n t·ª´ file Word."""
    doc = docx.Document(docx_path)
    full_text = []
    for para in doc.paragraphs:
        if para.text.strip():
            full_text.append(para.text)
    return "\n".join(full_text)

def ingest_document_to_qdrant(file_path: str):
    if not os.path.exists(file_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file t·∫°i: {file_path}")
        return

    file_extension = os.path.splitext(file_path)[1].lower()
    print(f"üìÑ ƒêang x·ª≠ l√Ω file {file_extension.upper()}: {os.path.basename(file_path)}...")
    
    all_chunks = []
    all_metadatas = []
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        separators=["\n\n", "\n", " ", ""]
    )

    try:
        # --- LU·ªíNG X·ª¨ L√ù PDF ---
        if file_extension == ".pdf":
            with fitz.open(file_path) as doc:
                for i, page in enumerate(doc):
                    page_text = page.get_text().strip()
                    if not page_text: continue
                    
                    chunks = text_splitter.split_text(page_text)
                    for chunk in chunks:
                        all_chunks.append(chunk)
                        all_metadatas.append({
                            "source": os.path.basename(file_path),
                            "page": i + 1,
                            "type": "pdf"
                        })

        # --- LU·ªíNG X·ª¨ L√ù WORD ---
        elif file_extension == ".docx":
            full_text = extract_text_from_word(file_path)
            if full_text:
                # V√¨ Word kh√¥ng c√≥ kh√°i ni·ªám trang v·∫≠t l√Ω nh∆∞ PDF, 
                # ta chia to√†n b·ªô vƒÉn b·∫£n v√† ƒë·ªÉ metadata page = 1 (ho·∫∑c theo section)
                chunks = text_splitter.split_text(full_text)
                for chunk in chunks:
                    all_chunks.append(chunk)
                    all_metadatas.append({
                        "source": os.path.basename(file_path),
                        "page": 1, 
                        "type": "docx"
                    })
        
        else:
            print(f"‚ö†Ô∏è ƒê·ªãnh d·∫°ng {file_extension} ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£.")
            return

    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc t√†i li·ªáu: {e}")
        return

    # 3. ƒê·∫©y v√†o Database
    if all_chunks:
        print(f"üß¨ ƒê√£ t·∫°o {len(all_chunks)} chunks.")
        print(f"üì° ƒêang n·∫°p v√†o Qdrant...")
        add_to_vector_db(chunks=all_chunks, metadatas=all_metadatas)
        print("‚úÖ Qu√° tr√¨nh n·∫°p d·ªØ li·ªáu ho√†n t·∫•t!")
    else:
        print("‚ö†Ô∏è Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n n√†o.")

if __name__ == "__main__":
    # Test th·ª≠ v·ªõi file Word c·ªßa b·∫°n
    SAMPLE_WORD = r"D:\Study-Agent\data\thuvienhoclieu.com-GA-Lich-su-9-CTST-ca-nam-hay.docx"
    ingest_document_to_qdrant(SAMPLE_WORD)